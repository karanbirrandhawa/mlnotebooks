{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Generator with RNNs\n",
    "\n",
    "## Recurrent Neural Network Based Language Models\n",
    "\n",
    "The RNNLM is useful in that it allows us to apply likelihoods to words and sentences, then lets us take it a step further by providing functionality in generating new text. \n",
    "\n",
    "### The Recurrent Neural Network\n",
    "\n",
    "A recurrent neural network differs from feedforward neural networks in that it has internal state with its units in the form of a directed cycle. That means that recurrent neural networks can take the input not just as the current input *but also the example prior to it*.  \n",
    "\n",
    "Thus recurrent neural networks have two sources of input: the present and the recent past. The decision at time step `t-1` will affect the decision at time step `t`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../images/RNN-unrolled.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-84d53878ae2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../images/RNN-unrolled.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../images/RNN-unrolled.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../images/RNN-unrolled.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram above shows on the right side that every iteration of the neural network will be passing its result to the next iteration of the neural network. Thus example $x_1$ will operate according to the values in example $x_1$ and the result $h_0$. This can be rolled up into the left-hand side as a feedback loop. \n",
    "\n",
    "This arrangement indicates that RNNs are especially suitable for dealing with examples as sequences. A lot of training data may consist of independent points but some training data (such as a paragraph of words) may be related by sequence. RNNs are very appropriate for dealing with that sort of data.\n",
    "\n",
    "Now that we know how they function we can think about how we might implement an RNN. We can't simply use backpropogation since a recurrent neural network has a cyclical dependency. A requirement of backpropogation is that  the neural network be acyclic so that it can clearly go from output back to input. With an RNN there is a cycle in order to feed the output back to input so that just won't cut it. \n",
    "\n",
    "**Backpropogation Through Time (BPTT)**\n",
    "\n",
    "This is a variation of backprogation to allow us to account for the fact that the RNN is cyclic. This [article](https://deeplearning4j.org/lstm) covers BPTT and how to fix various other problems commonly associated with recurrent neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Effectiveness\n",
    "\n",
    "The link [here](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) describes a little of how RNNs can be used effectively. Let's quickly do an overview of examples they provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='../images/diags.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that input vectors are red, output vectors are blue and green is the RNN's state. \n",
    "\n",
    "**One to One**. Vanilla approach to neural networks (i.e. without RNN).\n",
    "**One to Many**. Sequence output. One input vector will produce a sequence. Such as an image producing a sequence of words. \n",
    "**Many to One**. Sequence input. Such as taking a sequence of words and identifying sentiment (i.e. happy, sad, etc).\n",
    "**Many to Many**. Sequence input *and* sequence output. Such as taking a sequence of words in English and translating to a sequence of words in French.\n",
    "**Many to Many (2)**. Synced sequence input and output. Such as video classification where we label each frame of the video. \n",
    "\n",
    "There's also an interesting approach to using RNNs with models that are not necessarily sequenced in any way inherently discussed further in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNLM\n",
    "\n",
    "We can start by understanding that probability of a sentence that consists of worods $w_1,...w_n$ as:\n",
    "\n",
    "\\begin{align}\n",
    "P(w_1,...,w_n) = \\prod_{i=1}^n P(w_i \\space | \\space w_1,...,w_{i-1})\n",
    "\\end{align}\n",
    "\n",
    "The probability of a sentence is thus the probality that $w_2$ occurs given $w_1$ occurs, then $w_3$ occurs giveen $w_2$ and $w_1$ occurred and so on. This is useful as it provides us a scoring mechanism for likelihood of a word occurring. This also let's us approach the problem of generating new text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matplot lib setup\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM \n",
    "\n",
    "The recurrent neural network model we discussed above are extremely useful since they can use recent iterations in order to support the decision behind the current iteration. However, one flaw behind them is that in scenarios where the context we want to use for the current iteration is a huge amount of iterations behind we're going to result in a very slow RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/RNN-longtermdependencies.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get around this we can use LSTM (Long Short Term Memory) networks, which are designed specifically to solve the long-term dependency we outlined above.\n",
    "\n",
    "Take a standard RNN where layes $h_{t+1}$ and $h_{t-1}$ are seperated by a single layer $h_t$. Each layer in the neural network, $h_i$, will have a simple structure for its calculations i.e. $tanh$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/LSTM3-SimpleRNN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs introduce a vastly different structure for each layer, meant to help with long-term dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/LSTM3-chain.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a complex structure so let's try to get through it step by step.\n",
    "\n",
    "**The Cell State**\n",
    "\n",
    "The cell state is the horizontal line running straight through the top of the structure. It can be described as a conveyer belt as it runs straight through the entire chain. It's the reason that the LSTM can pass data so easily from a long, long time ago to the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/LSTM3-C-line.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can note that the cell state is actually editable, as can be seen by the two pink circles which we'll refer to as *gates*. \n",
    "\n",
    "#### Step by Step Walkthrough of an LSTM\n",
    "\n",
    "**$1$. Should I keep information in the cell state? The Forget Gate Layer.**\n",
    "\n",
    "This sigmoid layer will look at $h_{t-1}$ and $x_t$, then output a number of either $0$ and $1$ to every number in the cell state $C_{t-1}$. A $1$ will indicate \"completely keep this\" while a 0 will say \"completely get rid of this\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/LSTM3-focus-f.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$2$. Decide which new information to store in the cell state**.\n",
    "\n",
    "First we have a sigmoid layer called the \"input gate layer\" which is going to tell us whether or not we want to update the number. This will correspond to the Forget Gate Layer. Next, a $tanh$ layer will create a vector for the new candidate for the number in the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/LSTM3-focus-i.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the last step forward and update the old state $C_{t-1}$ with the new state $C_t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/LSTM3-focus-C.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we need to output the results of this layer. This is similar to how we did it in the ordinary RNN. We have a $tanh$ layer that will emit the output. However, prior to this we multiply by a sigmoid layer so we only output the parts we decide to output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../images/LSTM3-focus-o.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few variations to the LSTM above. Often in a project we find ourselves needing to use a different version of the LSTM. Regardless the core concept should be present in our minds so we can proceed in the implementation.\n",
    "\n",
    "From this point onwards it should be understood that most of what we do will borrow from [here](https://github.com/sherjilozair/char-rnn-tensorflow) as it provides a really good example of how to implement TensorFlow Recurrent Neural Networks and provides us a lot of pre-made functionality. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
