{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian Classifier\n",
    "\n",
    "The naive bayesian classifier is a simple probabilistic classifier based on applying Bayes Theorem with a naive assumption of independence between the feautures. \n",
    "\n",
    "## Bayesian Inference \n",
    "\n",
    "**Bayes Theorem**\n",
    "\n",
    "Sitting at the core of this classifier is Bayes theorem which operates as follows for two independent events $A$ and $B$ (given that $P(B) != 0 $).\n",
    "\n",
    "\\begin{align}\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
    "\\end{align}\n",
    "\n",
    "- $P(A|B)$ is the probability of event A given event B is true.\n",
    "- $P(B|A)$ is the probability of event B given event A is true.\n",
    "- $P(A)$ and $P(A)$ are the probabilies of events A and B independent of one another. \n",
    "\n",
    "**Bayesian Interpretation**\n",
    "\n",
    "For our naive bayes algorithm we'll interpret the theorem in a particular way. We'll identify probability as \"degree of belief\". We will then take a proposition $A$ and evidence $B$:\n",
    "\n",
    "- $P(A)$, the *prior*, is the inital degree of belief in $A$.\n",
    "- $P(A|B)$, the \"posterior\", is the degree of belief in $A$ having accounted for the degree of belieft in $B$.\n",
    "- $\\fract{P(B|A)}{P(B)}$ represents the support $B$ provides for $A$.\n",
    "\n",
    "**Alternative Form**\n",
    "\n",
    "\\begin{align}\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B|A) P(A) + P(B|-A) P(-A)}\n",
    "\\end{align}\n",
    "\n",
    "- $P(A)$ is the prior, the initial degree of belief in proposition $A$.\n",
    "- $P(-A)$ is the degree of belief *against* proposition $A$.\n",
    "- $P(B|A)$ is the degree of belief in evidence $B$ given $A$ is true.\n",
    "- $P(B|-A)$ is the degree of belief in evidence $B$ given $A$ is false.\n",
    "- $P(A|B)$ is the degree of belief in proposition $A$ given the evidence $B$.\n",
    "\n",
    "**Bayesian Inference**\n",
    "\n",
    "We can use Bayes Theorem to update the probability, the degree of belief of our proposition, as more evidence becomes available. \n",
    "\n",
    "We could have *n* input vectors $x_1,...,x_n$ and a desired output $y$. We want to identify:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x_1,...,x_n) = \\frac{P(y) P(x_1,...,x_n|y)}{P(x_1,...,x_n)}\n",
    "\\end{align}\n",
    "\n",
    "Let's recall that for two independent events $A$ and $B$, $P(A)P(B|A) = P(A)P(B)$. We can also note that since the joint probability mass function is the product of the marginals: $P(A,B) = P(A)P(B)$.\n",
    "\n",
    "Thus we can transform our numerator:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x_1,...,x_n) & = \\frac{P(y) P(x_1,...,x_n|y)}{P(x_1,...,x_n)} \\\\ \n",
    "& = \\frac{P(x_1,...,x_n, y)}{P(x_1,...,x_n)} \\\\\n",
    "& = \\frac{P(x_1|x_2,...,x_n, y)P(x_2,...,x_n, y)}{P(x_1,...,x_n)} \\space [note 1] \\\\\n",
    "& = \\frac{P(x_1|x_2,...,x_n, y)P(x_2|x_3...,x_n, y)P(x_3...,x_n, y)}{P(x_1,...,x_N)} \\\\\n",
    "& = \\frac{P(x_1|x_2,...,x_n, y)P(x_2|x_3...,x_n, y)...P(x_{n-1}|x_n, y)P(x_n|y)}{P(x_1,...,x_N)} \\\\\n",
    "& = \\frac{P(x_n|y) \\prod_{i=1}^n P(x_i | y)}{P(x_1,...,x_n)} \n",
    "\\end{align}\n",
    "\n",
    "- [note 1] Chain rule. \n",
    "\n",
    "If we take the numerator alone then we can understand the following relationship:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x_1,...,x_n) \\propto P(y) \\prod_{i=1}^n P(x_i | y)\n",
    "\\end{align}\n",
    "\n",
    "We can then finalize the formal classifier as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = arg \\space max \\space P(y) \\prod_{i=1}^n P(x_i | y)\n",
    "\\end{align}\n",
    "\n",
    "We use the [maximum a posteriori (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) in order to get the final result. \n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "We can identify three types of Naive Bayes models we'll be building in this notebook:\n",
    "\n",
    "1. **Gaussian**. Used in classification and assumes features follow a normal distribution.\n",
    "2. **Multinomial**. Used for discrete counts, i.e. text classification.\n",
    "3. **Bernoulli**. Also known as binomial model. Useful if features are binary.\n",
    "\n",
    "\n",
    "### Gaussian\n",
    "\n",
    "For this variation we assume the likelihood to be Gaussian:\n",
    "\n",
    "\\begin{align}\n",
    "P(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y^2}} exp(-\\frac{(x_i - \\mu_y)^2}{2 \\sigma_y^2})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" \n",
    "      % (iris.data.shape[0],(iris.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial\n",
    "\n",
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
